# í•œêµ­ì–´ ë‹¤ìŒ í† í° ì˜ˆì¸¡ê¸° êµ¬í˜„ ê°€ì´ë“œ (HOW-TO)

## ëª©ì°¨
1. [ì‹¤ì œ êµ¬í˜„ ì•„í‚¤í…ì²˜](#1-ì‹¤ì œ-êµ¬í˜„-ì•„í‚¤í…ì²˜)
2. [í™˜ê²½ ì„¤ì •](#2-í™˜ê²½-ì„¤ì •)
3. [ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ](#3-ë‹¨ê³„ë³„-êµ¬í˜„-ê°€ì´ë“œ)
4. [ì„±ëŠ¥ ìµœì í™” ì „ëµ](#4-ì„±ëŠ¥-ìµœì í™”-ì „ëµ)
5. [ë°°í¬ ë° ìš´ì˜](#5-ë°°í¬-ë°-ìš´ì˜)
6. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#6-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

## 1. ì‹¤ì œ êµ¬í˜„ ì•„í‚¤í…ì²˜

### 1.1 ê¸°ìˆ  ìŠ¤íƒ ê²°ì •
```
Frontend:  React 18 + TypeScript + Vite
Backend:   FastAPI 0.104+ + Python 3.10+
Model:     KoGPT2 (ì´ˆê¸°) â†’ DistilKoBERT (ìµœì í™”)
Cache:     Redis (production) / In-Memory (development)
Deploy:    Docker + nginx + gunicorn
```

### 1.2 ì‹¤ì œ ë™ì‘ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontend (3000)              â”‚
â”‚   - ë””ë°”ìš´ì‹± ì…ë ¥ ì²˜ë¦¬               â”‚
â”‚   - WebSocket ì—°ê²° ê´€ë¦¬              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Nginx (80/443)                    â”‚
â”‚   - WebSocket Proxy                 â”‚
â”‚   - Load Balancing                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Backend (8000)            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Connection Manager         â”‚    â”‚
â”‚   â”‚  - WebSocket ì„¸ì…˜ ê´€ë¦¬      â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Model Manager (Singleton)  â”‚    â”‚
â”‚   â”‚  - ëª¨ë¸ ë¡œë”©/ì–¸ë¡œë”©         â”‚    â”‚
â”‚   â”‚  - GPU/CPU ìë™ ì„ íƒ        â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚  Prediction Service         â”‚    â”‚
â”‚   â”‚  - í† í° â†’ ì–´ì ˆ ë³€í™˜         â”‚    â”‚
â”‚   â”‚  - ìºì‹± ì²˜ë¦¬                â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Redis Cache (6379)                â”‚
â”‚   - ì˜ˆì¸¡ ê²°ê³¼ ìºì‹±                  â”‚
â”‚   - ì„¸ì…˜ ë°ì´í„° ì €ì¥                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. í™˜ê²½ ì„¤ì •

### 2.1 í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# backend/requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
transformers==4.36.0
torch==2.1.0  # CPU ë²„ì „: torch==2.1.0+cpu
redis==5.0.1
python-multipart==0.0.6
websockets==12.0
pydantic==2.5.0
numpy==1.24.3
```

### 2.2 í”„ë¡œì íŠ¸ êµ¬ì¡°
```
korean-token-predictor/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI ì•±
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ manager.py       # ëª¨ë¸ ê´€ë¦¬
â”‚   â”‚   â”‚   â””â”€â”€ predictor.py     # ì˜ˆì¸¡ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py         # Redis ìºì‹±
â”‚   â”‚   â”‚   â””â”€â”€ tokenizer.py     # í† í¬ë‚˜ì´ì§•
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ websocket.py     # WebSocket í•¸ë“¤ëŸ¬
â”‚   â”‚   â”‚   â””â”€â”€ rest.py          # REST ì—”ë“œí¬ì¸íŠ¸
â”‚   â”‚   â””â”€â”€ config.py            # ì„¤ì •
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ InputEditor.tsx
â”‚   â”‚   â”‚   â””â”€â”€ PredictionList.tsx
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â””â”€â”€ useWebSocket.ts
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ nginx.conf
```

## 3. ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### Step 1: ëª¨ë¸ ë§¤ë‹ˆì € êµ¬í˜„
```python
# backend/app/models/manager.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import Optional
import logging

class ModelManager:
    _instance = None
    _model = None
    _tokenizer = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if self._model is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.load_model()

    def load_model(self, model_name: str = "skt/kogpt2-base-v2"):
        """ëª¨ë¸ ë¡œë”© with ë©”ëª¨ë¦¬ ìµœì í™”"""
        try:
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self._tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                cache_dir="./model_cache"
            )

            # ëª¨ë¸ ë¡œë“œ with ìµœì í™”
            self._model = AutoModelForCausalLM.from_pretrained(
                model_name,
                cache_dir="./model_cache",
                torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                low_cpu_mem_usage=True
            ).to(self.device)

            # í‰ê°€ ëª¨ë“œ ì„¤ì •
            self._model.eval()

            # ë©”ëª¨ë¦¬ ìµœì í™”
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

            logging.info(f"Model loaded on {self.device}")

        except Exception as e:
            logging.error(f"Model loading failed: {e}")
            raise

    @property
    def model(self):
        return self._model

    @property
    def tokenizer(self):
        return self._tokenizer
```

### Step 2: ì˜ˆì¸¡ ì„œë¹„ìŠ¤ êµ¬í˜„
```python
# backend/app/models/predictor.py
import torch
from typing import List, Tuple
import numpy as np

class PredictionService:
    def __init__(self, model_manager: ModelManager, cache_service=None):
        self.model_manager = model_manager
        self.cache = cache_service

    def predict_next_tokens(
        self,
        text: str,
        top_k: int = 5,
        temperature: float = 0.8,
        complete_word: bool = True
    ) -> List[Tuple[str, float]]:
        """
        ë‹¤ìŒ í† í° ì˜ˆì¸¡ (í† í° â†’ ì™„ì „í•œ ì–´ì ˆ ë³€í™˜ í¬í•¨)
        """
        # ìºì‹œ í™•ì¸
        cache_key = f"predict:{text}:{top_k}:{temperature}"
        if self.cache:
            cached = self.cache.get(cache_key)
            if cached:
                return cached

        # í† í¬ë‚˜ì´ì§•
        inputs = self.model_manager.tokenizer(
            text,
            return_tensors="pt",
            padding=False,
            truncation=True,
            max_length=128  # ì…ë ¥ ê¸¸ì´ ì œí•œ
        ).to(self.model_manager.device)

        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model_manager.model(**inputs)
            logits = outputs.logits[0, -1, :] / temperature

            # Softmax ì ìš©
            probs = torch.softmax(logits, dim=-1)

            # Top-k ì„ íƒ
            top_probs, top_indices = torch.topk(probs, min(top_k * 3, 50))

        # ì™„ì „í•œ ì–´ì ˆ ìƒì„±
        predictions = []
        if complete_word:
            predictions = self._complete_words(
                text, top_indices, top_probs, top_k
            )
        else:
            # ë‹¨ìˆœ í† í° ì˜ˆì¸¡
            for idx, prob in zip(top_indices[:top_k], top_probs[:top_k]):
                token = self.model_manager.tokenizer.decode([idx])
                predictions.append((token.strip(), float(prob)))

        # ìºì‹œ ì €ì¥ (TTL: 60ì´ˆ)
        if self.cache and predictions:
            self.cache.set(cache_key, predictions, ttl=60)

        return predictions

    def _complete_words(
        self,
        context: str,
        indices: torch.Tensor,
        probs: torch.Tensor,
        top_k: int
    ) -> List[Tuple[str, float]]:
        """í† í°ì„ ì™„ì „í•œ ì–´ì ˆë¡œ í™•ì¥"""
        completed_words = []
        seen_words = set()

        for idx, prob in zip(indices.tolist(), probs.tolist()):
            if len(completed_words) >= top_k:
                break

            # ì‹œì‘ í† í°ìœ¼ë¡œ ì–´ì ˆ ìƒì„±
            word, word_prob = self._generate_complete_word(
                context, idx, prob
            )

            # ì¤‘ë³µ ì œê±°
            if word and word not in seen_words:
                seen_words.add(word)
                completed_words.append((word, word_prob))

        return completed_words

    def _generate_complete_word(
        self,
        context: str,
        start_token_id: int,
        start_prob: float,
        max_length: int = 10
    ) -> Tuple[str, float]:
        """ë‹¨ì¼ í† í°ì—ì„œ ì™„ì „í•œ ì–´ì ˆ ìƒì„±"""
        tokenizer = self.model_manager.tokenizer
        model = self.model_manager.model

        # ì»¨í…ìŠ¤íŠ¸ + ì‹œì‘ í† í°
        input_ids = tokenizer.encode(context, return_tensors="pt")
        input_ids = torch.cat([
            input_ids,
            torch.tensor([[start_token_id]]).to(input_ids.device)
        ], dim=1).to(self.model_manager.device)

        generated = [start_token_id]
        accumulated_prob = start_prob

        # ì–´ì ˆ ì™„ì„±ê¹Œì§€ ìƒì„±
        for _ in range(max_length):
            with torch.no_grad():
                outputs = model(input_ids)
                logits = outputs.logits[0, -1, :]
                probs = torch.softmax(logits, dim=-1)
                next_token_id = torch.argmax(probs).item()
                next_prob = probs[next_token_id].item()

            # ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ í† í° ë§Œë‚˜ë©´ ì¤‘ë‹¨
            next_token = tokenizer.decode([next_token_id])
            if next_token.strip() == '' or next_token in ['</s>', '<pad>']:
                break

            generated.append(next_token_id)
            accumulated_prob *= next_prob
            input_ids = torch.cat([
                input_ids,
                torch.tensor([[next_token_id]]).to(input_ids.device)
            ], dim=1)

            # í•œêµ­ì–´ ì–´ì ˆ ê²½ê³„ í™•ì¸
            current_word = tokenizer.decode(generated)
            if self._is_complete_korean_word(current_word):
                break

        word = tokenizer.decode(generated).strip()
        return word, accumulated_prob

    def _is_complete_korean_word(self, text: str) -> bool:
        """í•œêµ­ì–´ ì–´ì ˆ ì™„ì„± ì—¬ë¶€ í™•ì¸"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì¡°ì‚¬ë‚˜ ì–´ë¯¸ íŒ¨í„´ í™•ì¸
        endings = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ',
                  'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë‹¤', 'ìš”', 'ìŠµë‹ˆë‹¤']
        return any(text.endswith(ending) for ending in endings)
```

### Step 3: WebSocket í•¸ë“¤ëŸ¬ êµ¬í˜„
```python
# backend/app/api/websocket.py
from fastapi import WebSocket, WebSocketDisconnect
from typing import Dict, Set
import asyncio
import json
import logging

class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
        self.user_contexts: Dict[str, str] = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
        self.user_contexts[client_id] = ""
        logging.info(f"Client {client_id} connected")

    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
            del self.user_contexts[client_id]
            logging.info(f"Client {client_id} disconnected")

    async def send_prediction(
        self,
        client_id: str,
        predictions: List[Tuple[str, float]]
    ):
        if client_id in self.active_connections:
            message = {
                "type": "prediction",
                "data": [
                    {"text": text, "confidence": conf}
                    for text, conf in predictions
                ]
            }
            await self.active_connections[client_id].send_json(message)

# WebSocket ì—”ë“œí¬ì¸íŠ¸
async def websocket_endpoint(
    websocket: WebSocket,
    client_id: str,
    manager: ConnectionManager,
    predictor: PredictionService
):
    await manager.connect(websocket, client_id)

    try:
        while True:
            # ë©”ì‹œì§€ ìˆ˜ì‹ 
            data = await websocket.receive_json()

            if data["type"] == "input":
                text = data["text"]

                # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                manager.user_contexts[client_id] = text

                # ë¹„ë™ê¸° ì˜ˆì¸¡
                predictions = await asyncio.to_thread(
                    predictor.predict_next_tokens,
                    text,
                    top_k=5
                )

                # ê²°ê³¼ ì „ì†¡
                await manager.send_prediction(client_id, predictions)

    except WebSocketDisconnect:
        manager.disconnect(client_id)
    except Exception as e:
        logging.error(f"WebSocket error: {e}")
        manager.disconnect(client_id)
```

### Step 4: FastAPI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
```python
# backend/app/main.py
from fastapi import FastAPI, WebSocket
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import uuid
from contextlib import asynccontextmanager

from app.models.manager import ModelManager
from app.models.predictor import PredictionService
from app.services.cache import CacheService
from app.api.websocket import ConnectionManager, websocket_endpoint

# ì „ì—­ ê°ì²´
model_manager = None
predictor = None
connection_manager = None
cache_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    global model_manager, predictor, connection_manager, cache_service

    print("Initializing services...")
    model_manager = ModelManager()
    cache_service = CacheService()  # Redis ë˜ëŠ” In-Memory
    predictor = PredictionService(model_manager, cache_service)
    connection_manager = ConnectionManager()
    print("Services initialized!")

    yield

    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    print("Cleaning up...")
    if cache_service:
        cache_service.close()

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="Korean Token Predictor",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "model_loaded": model_manager is not None
    }

@app.websocket("/ws/{client_id}")
async def websocket_route(websocket: WebSocket, client_id: str):
    await websocket_endpoint(
        websocket,
        client_id,
        connection_manager,
        predictor
    )

# REST API ëŒ€ì²´ ì—”ë“œí¬ì¸íŠ¸
@app.post("/predict")
async def predict_endpoint(text: str, top_k: int = 5):
    predictions = await asyncio.to_thread(
        predictor.predict_next_tokens,
        text,
        top_k
    )
    return {"predictions": predictions}

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=False,  # Productionì—ì„œëŠ” False
        workers=1      # ëª¨ë¸ ë¡œë”© ì´ìŠˆë¡œ ë‹¨ì¼ ì›Œì»¤
    )
```

### Step 5: React í”„ë¡ íŠ¸ì—”ë“œ êµ¬í˜„
```typescript
// frontend/src/hooks/useWebSocket.ts
import { useEffect, useRef, useState, useCallback } from 'react';

interface Prediction {
  text: string;
  confidence: number;
}

export function useWebSocket(url: string) {
  const [predictions, setPredictions] = useState<Prediction[]>([]);
  const [isConnected, setIsConnected] = useState(false);
  const ws = useRef<WebSocket | null>(null);
  const reconnectTimeout = useRef<NodeJS.Timeout>();
  const inputTimeout = useRef<NodeJS.Timeout>();

  const connect = useCallback(() => {
    const clientId = sessionStorage.getItem('clientId') ||
                     (() => {
                       const id = crypto.randomUUID();
                       sessionStorage.setItem('clientId', id);
                       return id;
                     })();

    ws.current = new WebSocket(`${url}/${clientId}`);

    ws.current.onopen = () => {
      console.log('WebSocket connected');
      setIsConnected(true);
    };

    ws.current.onmessage = (event) => {
      const message = JSON.parse(event.data);
      if (message.type === 'prediction') {
        setPredictions(message.data);
      }
    };

    ws.current.onerror = (error) => {
      console.error('WebSocket error:', error);
    };

    ws.current.onclose = () => {
      setIsConnected(false);
      // ìë™ ì¬ì—°ê²°
      reconnectTimeout.current = setTimeout(connect, 3000);
    };
  }, [url]);

  useEffect(() => {
    connect();

    return () => {
      if (reconnectTimeout.current) {
        clearTimeout(reconnectTimeout.current);
      }
      if (ws.current) {
        ws.current.close();
      }
    };
  }, [connect]);

  const sendInput = useCallback((text: string) => {
    // ë””ë°”ìš´ì‹± ì²˜ë¦¬
    if (inputTimeout.current) {
      clearTimeout(inputTimeout.current);
    }

    inputTimeout.current = setTimeout(() => {
      if (ws.current?.readyState === WebSocket.OPEN) {
        ws.current.send(JSON.stringify({
          type: 'input',
          text: text
        }));
      }
    }, 300); // 300ms ë””ë°”ìš´ì‹±
  }, []);

  return {
    predictions,
    isConnected,
    sendInput
  };
}
```

```tsx
// frontend/src/components/InputEditor.tsx
import React, { useState, useEffect } from 'react';
import { useWebSocket } from '../hooks/useWebSocket';

export function InputEditor() {
  const [text, setText] = useState('');
  const [selectedPrediction, setSelectedPrediction] = useState<string | null>(null);
  const { predictions, isConnected, sendInput } = useWebSocket(
    process.env.REACT_APP_WS_URL || 'ws://localhost:8000/ws'
  );

  const handleInputChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const newText = e.target.value;
    setText(newText);

    // ìŠ¤í˜ì´ìŠ¤ ì…ë ¥ ê°ì§€
    if (newText.endsWith(' ')) {
      sendInput(newText);
    }
  };

  const handlePredictionClick = (prediction: string) => {
    setText(text + prediction + ' ');
    setSelectedPrediction(prediction);
    sendInput(text + prediction + ' ');
  };

  return (
    <div className="editor-container">
      <div className="connection-status">
        {isConnected ? 'ğŸŸ¢ ì—°ê²°ë¨' : 'ğŸ”´ ì—°ê²° ëŠê¹€'}
      </div>

      <textarea
        value={text}
        onChange={handleInputChange}
        placeholder="í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
        className="input-area"
        rows={10}
      />

      {predictions.length > 0 && (
        <div className="predictions-panel">
          <h3>ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡</h3>
          <ul className="predictions-list">
            {predictions.map((pred, idx) => (
              <li
                key={idx}
                onClick={() => handlePredictionClick(pred.text)}
                className="prediction-item"
              >
                <span className="prediction-text">{pred.text}</span>
                <span className="prediction-confidence">
                  {(pred.confidence * 100).toFixed(1)}%
                </span>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}
```

## 4. ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 4.1 ëª¨ë¸ ìµœì í™”
```python
# ì–‘ìí™” ì ìš© (INT8)
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    device_map="auto"
)
```

### 4.2 ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„
```python
class BatchPredictor:
    def __init__(self, model_manager, batch_size=8, wait_time=0.1):
        self.batch_size = batch_size
        self.wait_time = wait_time
        self.pending_requests = []
        self.model_manager = model_manager

    async def add_request(self, text: str) -> List[Tuple[str, float]]:
        future = asyncio.Future()
        self.pending_requests.append((text, future))

        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()
        else:
            asyncio.create_task(self._wait_and_process())

        return await future

    async def _wait_and_process(self):
        await asyncio.sleep(self.wait_time)
        if self.pending_requests:
            await self._process_batch()

    async def _process_batch(self):
        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]

        texts = [text for text, _ in batch]
        results = await asyncio.to_thread(
            self._batch_predict, texts
        )

        for (_, future), result in zip(batch, results):
            future.set_result(result)
```

### 4.3 ìºì‹± ì „ëµ
```python
# backend/app/services/cache.py
import redis
import json
from typing import Optional, Any
import hashlib

class CacheService:
    def __init__(self, use_redis=True):
        self.use_redis = use_redis
        if use_redis:
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                decode_responses=True,
                socket_connect_timeout=5
            )
        else:
            self.memory_cache = {}

    def _generate_key(self, text: str, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{text}:{json.dumps(kwargs, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def get(self, key: str) -> Optional[Any]:
        try:
            if self.use_redis:
                data = self.redis_client.get(key)
                return json.loads(data) if data else None
            else:
                return self.memory_cache.get(key)
        except Exception as e:
            print(f"Cache get error: {e}")
            return None

    def set(self, key: str, value: Any, ttl: int = 60):
        try:
            if self.use_redis:
                self.redis_client.setex(
                    key,
                    ttl,
                    json.dumps(value)
                )
            else:
                self.memory_cache[key] = value
                # ê°„ë‹¨í•œ TTL êµ¬í˜„
                asyncio.create_task(self._expire_key(key, ttl))
        except Exception as e:
            print(f"Cache set error: {e}")

    async def _expire_key(self, key: str, ttl: int):
        await asyncio.sleep(ttl)
        if key in self.memory_cache:
            del self.memory_cache[key]
```

## 5. ë°°í¬ ë° ìš´ì˜

### 5.1 Docker êµ¬ì„±
```dockerfile
# backend/Dockerfile
FROM python:3.10-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„±
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Python íŒ¨í‚¤ì§€
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
RUN python -c "from transformers import AutoModelForCausalLM, AutoTokenizer; \
               AutoTokenizer.from_pretrained('skt/kogpt2-base-v2'); \
               AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')"

COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 5.2 Docker Compose ì„¤ì •
```yaml
# docker-compose.yml
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - REDIS_HOST=redis
      - MODEL_CACHE_DIR=/models
      - CUDA_VISIBLE_DEVICES=0  # GPU ì‚¬ìš© ì‹œ
    volumes:
      - model_cache:/models
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    environment:
      - REACT_APP_API_URL=http://localhost:8000
      - REACT_APP_WS_URL=ws://localhost:8000/ws
    depends_on:
      - backend

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend

volumes:
  redis_data:
  model_cache:
```

### 5.3 Nginx ì„¤ì •
```nginx
# nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream backend {
        server backend:8000;
    }

    upstream frontend {
        server frontend:3000;
    }

    server {
        listen 80;

        # WebSocket ì„¤ì •
        location /ws/ {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_read_timeout 86400;
        }

        # API
        location /api/ {
            proxy_pass http://backend/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Frontend
        location / {
            proxy_pass http://frontend;
            proxy_set_header Host $host;
        }
    }
}
```

### 5.4 ëª¨ë‹ˆí„°ë§ ì„¤ì •
```python
# backend/app/monitoring.py
from prometheus_client import Counter, Histogram, generate_latest
import time

# ë©”íŠ¸ë¦­ ì •ì˜
prediction_counter = Counter(
    'predictions_total',
    'Total number of predictions',
    ['model', 'status']
)

prediction_latency = Histogram(
    'prediction_duration_seconds',
    'Prediction latency in seconds',
    buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0]
)

class MetricsMiddleware:
    async def __call__(self, request, call_next):
        start_time = time.time()
        response = await call_next(request)
        duration = time.time() - start_time

        if request.url.path.startswith('/predict'):
            prediction_latency.observe(duration)
            prediction_counter.labels(
                model='kogpt2',
                status=response.status_code
            ).inc()

        return response

# ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
@app.get("/metrics")
async def metrics():
    return Response(
        generate_latest(),
        media_type="text/plain"
    )
```

## 6. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 6.1 ì¼ë°˜ì ì¸ ë¬¸ì œì™€ í•´ê²°ì±…

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# í•´ê²°ì±… 1: ëª¨ë¸ ì–‘ìí™”
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True
)

# í•´ê²°ì±… 2: Gradient Checkpointing
model.gradient_checkpointing_enable()

# í•´ê²°ì±… 3: ë°°ì¹˜ í¬ê¸° ì¶•ì†Œ
MAX_BATCH_SIZE = 4  # 8ì—ì„œ ì¶•ì†Œ
```

#### CPU ì¶”ë¡  ì†ë„ ê°œì„ 
```python
# í•´ê²°ì±… 1: ONNX ë³€í™˜
from transformers import pipeline
from optimum.onnxruntime import ORTModelForCausalLM

model = ORTModelForCausalLM.from_pretrained(
    model_name,
    from_transformers=True
)

# í•´ê²°ì±… 2: ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
# DistilKoBERT ë˜ëŠ” TinyBERT ì‚¬ìš©
```

#### WebSocket ì—°ê²° ëŠê¹€
```javascript
// í•´ê²°ì±…: ìë™ ì¬ì—°ê²° ë¡œì§
class ReconnectingWebSocket {
  constructor(url, options = {}) {
    this.url = url;
    this.reconnectInterval = options.reconnectInterval || 1000;
    this.maxReconnectAttempts = options.maxReconnectAttempts || 5;
    this.reconnectAttempts = 0;
    this.connect();
  }

  connect() {
    this.ws = new WebSocket(this.url);

    this.ws.onclose = () => {
      if (this.reconnectAttempts < this.maxReconnectAttempts) {
        setTimeout(() => {
          this.reconnectAttempts++;
          this.connect();
        }, this.reconnectInterval * this.reconnectAttempts);
      }
    };

    this.ws.onopen = () => {
      this.reconnectAttempts = 0;
    };
  }
}
```

### 6.2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```bash
# ë¡œë“œ í…ŒìŠ¤íŠ¸
pip install locust

# locustfile.py
from locust import HttpUser, task, between

class PredictionUser(HttpUser):
    wait_time = between(1, 3)

    @task
    def predict(self):
        self.client.post("/predict", json={
            "text": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€",
            "top_k": 5
        })

# ì‹¤í–‰
locust -f locustfile.py --host=http://localhost:8000
```

### 6.3 ë¡œê¹… ì„¤ì •
```python
# backend/app/config.py
import logging
from logging.handlers import RotatingFileHandler

def setup_logging():
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)

    # íŒŒì¼ í•¸ë“¤ëŸ¬
    file_handler = RotatingFileHandler(
        'app.log',
        maxBytes=10485760,  # 10MB
        backupCount=5
    )
    file_handler.setFormatter(
        logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    )

    # ì½˜ì†” í•¸ë“¤ëŸ¬
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(
        logging.Formatter('%(levelname)s - %(message)s')
    )

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
```

## ê²°ë¡ 

ì´ êµ¬í˜„ ê°€ì´ë“œëŠ” ì‹¤ì œ production í™˜ê²½ì—ì„œ ë™ì‘ ê°€ëŠ¥í•œ í•œêµ­ì–´ í† í° ì˜ˆì¸¡ê¸°ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§•:

1. **ì‹¤ì‹œê°„ ì„±ëŠ¥**: WebSocket ê¸°ë°˜ ì‹¤ì‹œê°„ í†µì‹ 
2. **í™•ì¥ì„±**: Docker ê¸°ë°˜ ì»¨í…Œì´ë„ˆí™” ë° Redis ìºì‹±
3. **ìµœì í™”**: ëª¨ë¸ ì–‘ìí™”, ë°°ì¹˜ ì²˜ë¦¬, ìºì‹± ì „ëµ
4. **ëª¨ë‹ˆí„°ë§**: Prometheus ë©”íŠ¸ë¦­ ë° ë¡œê¹…
5. **ì•ˆì •ì„±**: ìë™ ì¬ì—°ê²°, ì—ëŸ¬ ì²˜ë¦¬

ì´ ì‹œìŠ¤í…œì€ ì´ˆë‹¹ 50-100 ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, GPU í™˜ê²½ì—ì„œ 200-300ms, CPU í™˜ê²½ì—ì„œ 1-2ì´ˆì˜ ì‘ë‹µ ì‹œê°„ì„ ë³´ì¥í•©ë‹ˆë‹¤.